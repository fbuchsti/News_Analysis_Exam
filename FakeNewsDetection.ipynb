{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAKE NEWS DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains all relevant computations and investigations for the exam in DS821: News and Market Sentiment Analysis. The structure is predominantly corresponding to the structure of the report. However, most section is labeled with the number corresponding to the section of the report for transparency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for specific dependencies\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. DATA LOAD AND INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "fake = pd.read_csv(\"/Users/FrederikkeB/Documents/GitHub/News_Analysis_Exam/data/Fake.csv\") \n",
    "true = pd.read_csv(\"/Users/FrederikkeB/Documents/GitHub/News_Analysis_Exam/data/True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "fake.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "true.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign class labels to each dataframe\n",
    "true[\"label\"] = 0\n",
    "fake[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to two dataframes together \n",
    "df = pd.concat([fake, true], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect data frame\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df.duplicated(subset=[\"text\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates \n",
    "len_before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"text\"], keep=\"first\")\n",
    "len_after = len(df)\n",
    "\n",
    "print(\"Duplicates removed:\", len_before - len_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect how many articles comes from reuters\n",
    "df[\"starts_with_reuters\"] = df[\"text\"].str.lower().str.contains(\n",
    "    r\"^.*\\(\\s*reuters\\s*\\)\\s*-\", regex=True, na=False\n",
    ")\n",
    "# divide by class\n",
    "df.groupby(\"label\")[\"starts_with_reuters\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove potential metadata in the beginning of each body text\n",
    "def remove_leading_metadata(text):\n",
    "    pattern = r\"^[A-Z\\s\\/]+\\s*\\([A-Za-z]+\\)\\s*[–—-]\\s+\"\n",
    "    return re.sub(pattern, \"\", text).strip()\n",
    "\n",
    "# run on text column\n",
    "df[\"text_clean\"] = df[\"text\"].apply(remove_leading_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "df[\"metadata_removed\"] = df[\"text\"] != df[\"text_clean\"]\n",
    "df.groupby(\"label\")[\"metadata_removed\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the occurence of url in articles\n",
    "df[\"url_count\"] = df[\"text\"].str.count(r\"http[s]?://\")\n",
    "\n",
    "# print the difference between classes\n",
    "df.groupby(\"label\")[\"url_count\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect urls context in articles\n",
    "def url_context(text, window=5):\n",
    "    tokens = text.split()\n",
    "    contexts = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.startswith(\"http\"):\n",
    "            start = max(i - window, 0)\n",
    "            end = min(i + window + 1, len(tokens))\n",
    "            contexts.append(\" \".join(tokens[start:end]))\n",
    "    return contexts\n",
    "\n",
    "df[\"url_contexts\"] = df[\"text\"].apply(url_context)\n",
    "\n",
    "# print observations including urls\n",
    "fake_contexts = df[df[\"label\"] == 1][\"url_contexts\"].explode().dropna()\n",
    "fake_contexts.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove web related entities from body text \n",
    "def remove_urls_and_html(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)   \n",
    "    text = re.sub(r\"&\\w+;\", \" \", text)            \n",
    "    return text\n",
    "\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(remove_urls_and_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define web specific noise\n",
    "noise = {\n",
    "    \"https\", \"http\", \"www\", \"amp\", \"quot\", \"cdata\", \"js\",\n",
    "    \"pic\", \"youtu\", \"flickr\", \"getty\", \"wikimedia\",\n",
    "    \"screenshot\", \"src\", \"createelement\", \"getelementbyid\",\n",
    "    \"getelementsbytagname\", \"parentnode\", \"insertbefore\",\n",
    "    \"jssdk\", \"xfbml\", \"filessupport\", \"21wire\", \n",
    "}\n",
    "\n",
    "# translate to regex\n",
    "noise_pattern = re.compile(\n",
    "    r\"\\b(\" + \"|\".join(map(re.escape, noise)) + r\")\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# remove noise\n",
    "def remove_noise(text):\n",
    "    return noise_pattern.sub(\"\", text)\n",
    "\n",
    "# apply to cleaned text\n",
    "df[\"text_clean\"] = df[\"text_clean\"].apply(remove_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase body text\n",
    "df[\"text_clean\"] = df[\"text_clean\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "stopwords = set(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function combining each tokenization step\n",
    "def tokenize(text):\n",
    "\n",
    "    # extract alphabetic tokens\n",
    "    tokens = re.findall(r\"[a-zA-Z]+\", text)\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "\n",
    "    # remove tokens with less than 2 characters\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# apply to cleaned text \n",
    "df[\"tokens\"] = df[\"text_clean\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect clean dataframe\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LENGTH AND STYLE DIFFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check basic properties\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nClass proportions:\")\n",
    "print(df[\"label\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic length features\n",
    "df[\"char_count\"] = df[\"text_clean\"].str.len()\n",
    "df[\"word_count\"] = df[\"text_clean\"].str.split().str.len()\n",
    "\n",
    "# summary statistics per class\n",
    "length_stats = df.groupby(\"label\")[[\"char_count\", \"word_count\"]].agg(\n",
    "    [\"mean\", \"median\", \"std\"]\n",
    ")\n",
    "\n",
    "# print results\n",
    "length_stats.index = [\"True\", \"Fake\"]\n",
    "length_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function extracting most used words in each class\n",
    "def top_words_from_tokens(token_series, n=100):\n",
    "    tokens = [t for tokens in token_series for t in tokens]\n",
    "    return Counter(tokens).most_common(n)\n",
    "\n",
    "# apply function and store in each list\n",
    "top_true = top_words_from_tokens(df[df[\"label\"] == 0][\"tokens\"], 100)\n",
    "top_fake = top_words_from_tokens(df[df[\"label\"] == 1][\"tokens\"], 100)\n",
    "\n",
    "# print\n",
    "print(\"Top words – True news:\")\n",
    "print(top_true)\n",
    "\n",
    "print(\"\\nTop words – Fake news:\")\n",
    "print(top_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SENTITMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FINBERT SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define FinBERT-model\n",
    "finbert_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    device=\"mps\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function extracting scores from model\n",
    "def finbert_score(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return np.nan   \n",
    "\n",
    "    out = finbert_model(text)[0]\n",
    "    label = out[\"label\"]\n",
    "    score = out[\"score\"]\n",
    "\n",
    "    if label == \"positive\":\n",
    "        return score\n",
    "    elif label == \"negative\":\n",
    "        return -score\n",
    "    else:  # neutral\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate scores to labels\n",
    "def finbert_label(score, eps=0.1):\n",
    "    if pd.isna(score):\n",
    "        return np.nan\n",
    "    if score > eps:\n",
    "        return \"positive\"\n",
    "    if score < -eps:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a subset  \n",
    "finbert_subset = (\n",
    "    df\n",
    "    .groupby(\"label\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=2000, random_state=42))\n",
    ")\n",
    "\n",
    "# apply model to subset \n",
    "finbert_subset[\"finbert_score\"] = finbert_subset[\"text_clean\"].apply(finbert_score)\n",
    "finbert_subset[\"finbert_label\"] = finbert_subset[\"finbert_score\"].apply(\n",
    "    lambda x: finbert_label(x, eps=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "finbert_subset[\"finbert_label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results by class \n",
    "finbert_by_class = (\n",
    "    finbert_subset\n",
    "    .groupby(\"label\")[\"finbert_label\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    ")\n",
    "\n",
    "finbert_by_class.index = [\"True news\", \"Fake news\"]\n",
    "finbert_by_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROBERTA SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RoBERTa-model \n",
    "roberta_model = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=\"mps\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function extracting scores from model\n",
    "def roberta_score(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return np.nan   \n",
    "\n",
    "    out = roberta_model(text)[0]\n",
    "    label = out[\"label\"]\n",
    "    score = out[\"score\"]\n",
    "\n",
    "    if label == \"positive\":\n",
    "        return score\n",
    "    elif label == \"negative\":\n",
    "        return -score\n",
    "    else:  # neutral\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate scores to labels\n",
    "def roberta_label(score, eps=0.1):\n",
    "    if pd.isna(score):\n",
    "        return np.nan\n",
    "    if score > eps:\n",
    "        return \"positive\"\n",
    "    if score < -eps:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a subset\n",
    "roberta_subset = (\n",
    "    df\n",
    "    .groupby(\"label\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=2000, random_state=42))\n",
    ")\n",
    "\n",
    "# apply model to subset\n",
    "roberta_subset[\"roberta_score\"] = roberta_subset[\"text_clean\"].apply(roberta_score)\n",
    "roberta_subset[\"roberta_label\"] = roberta_subset[\"roberta_score\"].apply(\n",
    "    lambda x: roberta_label(x, eps=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "roberta_subset[\"roberta_label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results by class \n",
    "roberta_by_class = (\n",
    "    roberta_subset\n",
    "    .groupby(\"label\")[\"roberta_label\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    ")\n",
    "\n",
    "roberta_by_class.index = [\"True news\", \"Fake news\"]\n",
    "roberta_by_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 VOCABULARY CONTRASTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function counting tokens in a series of token lists\n",
    "def count_tokens(token_series):\n",
    "    return Counter([t for tokens in token_series for t in tokens])\n",
    "\n",
    "\n",
    "# token counts per class\n",
    "true_counts = count_tokens(df[df[\"label\"] == 0][\"tokens\"])\n",
    "fake_counts = count_tokens(df[df[\"label\"] == 1][\"tokens\"])\n",
    "\n",
    "min_freq = 50  \n",
    "\n",
    "\n",
    "# define a shared vocabulary of tokens appearing sufficiently often in both classes\n",
    "vocab = {\n",
    "    w for w in fake_counts\n",
    "    if fake_counts[w] >= min_freq and true_counts.get(w, 0) >= min_freq\n",
    "}\n",
    "\n",
    "\n",
    "# find words more characteristic of fake news\n",
    "rel_fake = {\n",
    "    w: math.log((fake_counts[w] + 1) / (true_counts[w] + 1))\n",
    "    for w in vocab\n",
    "}\n",
    "# sort values\n",
    "top_fake = sorted(\n",
    "    rel_fake.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:50]\n",
    "\n",
    "print(\"Words more characteristic of Fake news:\\n\")\n",
    "for w, s in top_fake:\n",
    "    print(f\"{w:<15} {s:.2f}\")\n",
    "\n",
    "\n",
    "# find words more characteristic of true news\n",
    "rel_true = {\n",
    "    w: math.log((true_counts[w] + 1) / (fake_counts[w] + 1))\n",
    "    for w in vocab\n",
    "}\n",
    "# sort values\n",
    "top_true = sorted(\n",
    "    rel_true.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:50]\n",
    "\n",
    "print(\"\\nWords more characteristic of True news:\\n\")\n",
    "for w, s in top_true:\n",
    "    print(f\"{w:<15} {s:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOPIC MODELLING: LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = df[df[\"label\"] == 0]\n",
    "df_fake = df[df[\"label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lda(token_lists, num_topics=7, no_below=20, no_above=0.9):\n",
    "    \n",
    "    # Create dictionary\n",
    "    dictionary = Dictionary(token_lists)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    \n",
    "    # Create BoW corpus\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in token_lists]\n",
    "    \n",
    "    # Train LDA\n",
    "    lda = LdaModel(\n",
    "        corpus=corpus_bow,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        passes=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return lda, dictionary, corpus_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_true, dict_true, corpus_true = run_lda(\n",
    "    df_true[\"tokens\"].tolist(),\n",
    "    num_topics=7\n",
    ")\n",
    "\n",
    "lda_fake, dict_fake, corpus_fake = run_lda(\n",
    "    df_fake[\"tokens\"].tolist(),\n",
    "    num_topics=7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Topics – True news\")\n",
    "for t in lda_true.print_topics(num_words=10):\n",
    "    print(t)\n",
    "\n",
    "print(\"\\nTopics – Fake news\")\n",
    "for t in lda_fake.print_topics(num_words=10):\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coherence score for each class\n",
    "coh_true = CoherenceModel(\n",
    "    model=lda_true,\n",
    "    texts=df_true[\"tokens\"].tolist(),\n",
    "    dictionary=dict_true,\n",
    "    coherence=\"c_v\"\n",
    ").get_coherence()\n",
    "\n",
    "coh_fake = CoherenceModel(\n",
    "    model=lda_fake,\n",
    "    texts=df_fake[\"tokens\"].tolist(),\n",
    "    dictionary=dict_fake,\n",
    "    coherence=\"c_v\"\n",
    ").get_coherence()\n",
    "\n",
    "coh_true, coh_fake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for all classification models\n",
    "X = df[\"text_clean\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# split traning and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 NAÏVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nb-classifier\n",
    "nb_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        max_df=0.9,\n",
    "        min_df=50\n",
    "    )),\n",
    "    (\"nb\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# define cross validation\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"f1\": \"f1\"\n",
    "}\n",
    "# run cross validation\n",
    "cv_results = cross_validate(\n",
    "    nb_pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see results for cross validation\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "cv_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on whole training set\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# run predictions on test set\n",
    "y_test_pred = nb_pipeline.predict(X_test)\n",
    "\n",
    "# print results\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    target_names=[\"True news\", \"Fake news\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXAMINE IMPORTANT WORDS FOR NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get components from pipeline\n",
    "vectorizer = nb_pipeline.named_steps[\"tfidf\"]\n",
    "nb_model = nb_pipeline.named_steps[\"nb\"]\n",
    "\n",
    "# extract words\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log probabilities per class\n",
    "log_probs = nb_model.feature_log_prob_\n",
    "\n",
    "# create dataframe for results\n",
    "df_log_probs = pd.DataFrame(\n",
    "    log_probs.T,\n",
    "    index=feature_names,\n",
    "    columns=[\"True news\", \"Fake news\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the difference between classes\n",
    "df_log_probs[\"log_odds_fake_vs_true\"] = (\n",
    "    df_log_probs[\"Fake news\"] - df_log_probs[\"True news\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top words for fake news\n",
    "top_fake_nb = (\n",
    "    df_log_probs\n",
    "    .sort_values(\"log_odds_fake_vs_true\", ascending=False)\n",
    "    .head(50)\n",
    ")\n",
    "\n",
    "# top words for true news\n",
    "top_true_nb = (\n",
    "    df_log_probs\n",
    "    .sort_values(\"log_odds_fake_vs_true\", ascending=True)\n",
    "    .head(50)\n",
    ")\n",
    "\n",
    "print(\"Top words indicating Fake news:\\n\")\n",
    "print(top_fake_nb[[\"log_odds_fake_vs_true\"]])\n",
    "\n",
    "print(\"\\nTop words indicating True news:\\n\")\n",
    "print(top_true_nb[[\"log_odds_fake_vs_true\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 CENTROID-BASED CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# embed training data\n",
    "X_train_embeddings = embedding_model.encode(\n",
    "    X_train.tolist(),\n",
    "    show_progress_bar=True\n",
    ")\n",
    "# convert targets to array\n",
    "y_train_array = y_train.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class-centroids\n",
    "true_centroid = X_train_embeddings[y_train_array == 0].mean(axis=0)\n",
    "fake_centroid = X_train_embeddings[y_train_array == 1].mean(axis=0)\n",
    "\n",
    "# reshape for cosine similarity\n",
    "true_centroid = true_centroid.reshape(1, -1)\n",
    "fake_centroid = fake_centroid.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed test data\n",
    "X_test_embeddings = embedding_model.encode(\n",
    "    X_test.tolist(),\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classification logic\n",
    "def centroid_predict(embeddings, true_centroid, fake_centroid):\n",
    "    sim_true = cosine_similarity(embeddings, true_centroid).flatten()\n",
    "    sim_fake = cosine_similarity(embeddings, fake_centroid).flatten()\n",
    "    \n",
    "    # predict class with highest similarity\n",
    "    return np.where(sim_fake > sim_true, 1, 0)\n",
    "\n",
    "# predict on test set \n",
    "y_pred_centroid = centroid_predict(\n",
    "    X_test_embeddings,\n",
    "    true_centroid,\n",
    "    fake_centroid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred_centroid,\n",
    "    target_names=[\"True news\", \"Fake news\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate distance between class centroids \n",
    "centroid_similarity = cosine_similarity(true_centroid, fake_centroid)[0, 0]\n",
    "print(\"Cosine similarity between class centroids:\", centroid_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity to centroids for test set\n",
    "sim_true = cosine_similarity(X_test_embeddings, true_centroid).flatten()\n",
    "sim_fake = cosine_similarity(X_test_embeddings, fake_centroid).flatten()\n",
    "\n",
    "analysis_df = pd.DataFrame({\n",
    "    \"text\": X_test.values,\n",
    "    \"true_label\": y_test.values,\n",
    "    \"sim_true\": sim_true,\n",
    "    \"sim_fake\": sim_fake,\n",
    "    \"pred_label\": y_pred_centroid\n",
    "})\n",
    "\n",
    "# compute margin to both classes for each observation \n",
    "analysis_df[\"margin\"] = analysis_df[\"sim_fake\"] - analysis_df[\"sim_true\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very confident fake predictions\n",
    "analysis_df.sort_values(\"margin\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very confident true predictions\n",
    "analysis_df.sort_values(\"margin\", ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified articles\n",
    "analysis_df[analysis_df[\"true_label\"] != analysis_df[\"pred_label\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ZERO-SHOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load zero-shot classifier\n",
    "zero_shot = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=\"mps\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels \n",
    "candidate_labels = [\"fake news\", \"real news\"]\n",
    "\n",
    "# extract subset\n",
    "subset_size = 500\n",
    "\n",
    "X_test_subset = X_test.sample(\n",
    "    n=subset_size,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_test_subset = y_test.loc[X_test_subset.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define decision logic\n",
    "def zero_shot_predict(texts):\n",
    "    outputs = zero_shot(texts, candidate_labels)\n",
    "    return np.array([\n",
    "        1 if o[\"labels\"][0] == \"fake news\" else 0\n",
    "        for o in outputs\n",
    "    ])\n",
    "\n",
    "y_pred_zs = zero_shot_predict(X_test_subset.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(classification_report(\n",
    "    y_test_subset,\n",
    "    y_pred_zs,\n",
    "    target_names=[\"True news\", \"Fake news\"]\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
